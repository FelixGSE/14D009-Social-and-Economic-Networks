}
compute.states <- function(  N  = 100,
x0 = c(3,2),
D  = diag(2) * c(0.5,0.2),
R  = diag(2) * c( 2, 1 ) ,
A  = matrix( c(0,3,0,0), 2, 2),
B  = matrix( c(4,2,0,1), 2, 2),
C  = c( 5, 0 ) )
{
# Set mean for disturbance
mu      <- c( 0 , 0 )
# Initialize storage objets and set corresponding starting conditions
x       <- matrix( NA , nrow = N , ncol = 2 )
x[1,]   <- x0
Q       <- C %*% t(C)
K       <- list()
K[[N]]  <- Q
L       <- list()
# Compute K and L matrices
for (k in (N-1):1){
K[[k]] <-   t(A) %*% ( K[[(k+1)]] - K[[(k+1)]] %*% B %*% solve( t(B) %*% K[[(k+1)]] %*% B + R ) %*% t(B) %*% K[[(k+1)]] ) %*% A + Q
L[[k]] <- - solve( t(B) %*% K[[(k+1)]] %*% B + R) %*% t(B) %*% K[[(k+1)]] %*% A
}
# Solve for the states
w      <- rmvnorm( 100 , mean = mu , sigma = D )
for( k in 2:N ){
u.temp <- L[[(k - 1)]] %*% x[(k-1),]
w      <- rmvnorm( 1 , mean = mu , sigma = D )
x[k,]  <- A %*% AM( x[(k-1),] ) + B %*% u.temp + t(AM(w[(k-1),]))
}
}
compute.states <- function(  N  = 100,
x0 = c(3,2),
D  = diag(2) * c(0.5,0.2),
R  = diag(2) * c( 2, 1 ) ,
A  = matrix( c(0,3,0,0), 2, 2),
B  = matrix( c(4,2,0,1), 2, 2),
C  = c( 5, 0 ) )
{
# Set mean for disturbance
mu      <- c( 0 , 0 )
# Initialize storage objets and set corresponding starting conditions
x       <- matrix( NA , nrow = N , ncol = 2 )
x[1,]   <- x0
Q       <- C %*% t(C)
K       <- list()
K[[N]]  <- Q
L       <- list()
# Compute K and L matrices
for (k in (N-1):1){
K[[k]] <-   t(A) %*% ( K[[(k+1)]] - K[[(k+1)]] %*% B %*% solve( t(B) %*% K[[(k+1)]] %*% B + R ) %*% t(B) %*% K[[(k+1)]] ) %*% A + Q
L[[k]] <- - solve( t(B) %*% K[[(k+1)]] %*% B + R) %*% t(B) %*% K[[(k+1)]] %*% A
}
# Solve for the states
w      <- rmvnorm( 100 , mean = mu , sigma = D )
for( k in 2:N ){
u.temp <- L[[(k - 1)]] %*% x[(k-1),]
w      <- rmvnorm( 1 , mean = mu , sigma = D )
x[k,]  <- A %*% AM( x[(k-1),] ) + B %*% u.temp + t(AM(w[(k-1),]))
}
}
}
compute.states <- function(  N  = 100,
x0 = c(3,2),
D  = diag(2) * c(0.5,0.2),
R  = diag(2) * c( 2, 1 ) ,
A  = matrix( c(0,3,0,0), 2, 2),
B  = matrix( c(4,2,0,1), 2, 2),
C  = c( 5, 0 ) )
{
# Set mean for disturbance
mu      <- c( 0 , 0 )
# Initialize storage objets and set corresponding starting conditions
x       <- matrix( NA , nrow = N , ncol = 2 )
x[1,]   <- x0
Q       <- C %*% t(C)
K       <- list()
K[[N]]  <- Q
L       <- list()
# Compute K and L matrices
for (k in (N-1):1){
K[[k]] <-   t(A) %*% ( K[[(k+1)]] - K[[(k+1)]] %*% B %*% solve( t(B) %*% K[[(k+1)]] %*% B + R ) %*% t(B) %*% K[[(k+1)]] ) %*% A + Q
L[[k]] <- - solve( t(B) %*% K[[(k+1)]] %*% B + R) %*% t(B) %*% K[[(k+1)]] %*% A
}
# Solve for the states
w      <- rmvnorm( 100 , mean = mu , sigma = D )
for( k in 2:N ){
u.temp <- L[[(k - 1)]] %*% x[(k-1),]
w      <- rmvnorm( 1 , mean = mu , sigma = D )
x[k,]  <- A %*% AM( x[(k-1),] ) + B %*% u.temp + t(AM(w[(k-1),]))
}
}
compute.states
compute.states()
compute.states <- function(  N  = 100,
x0 = c(3,2),
D  = diag(2) * c(0.5,0.2),
R  = diag(2) * c( 2, 1 ) ,
A  = matrix( c(0,3,0,0), 2, 2),
B  = matrix( c(4,2,0,1), 2, 2),
C  = c( 5, 0 ) )
{
# Set mean for disturbance
mu      <- c( 0 , 0 )
# Initialize storage objets and set corresponding starting conditions
x       <- matrix( NA , nrow = N , ncol = 2 )
x[1,]   <- x0
Q       <- C %*% t(C)
K       <- list()
K[[N]]  <- Q
L       <- list()
# Compute K and L matrices
for (k in (N-1):1){
K[[k]] <-   t(A) %*% ( K[[(k+1)]] - K[[(k+1)]] %*% B %*% solve( t(B) %*% K[[(k+1)]] %*% B + R ) %*% t(B) %*% K[[(k+1)]] ) %*% A + Q
L[[k]] <- - solve( t(B) %*% K[[(k+1)]] %*% B + R) %*% t(B) %*% K[[(k+1)]] %*% A
}
# Solve for the states
w      <- rmvnorm( 100 , mean = mu , sigma = D )
for( k in 2:N ){
u.temp <- L[[(k - 1)]] %*% x[(k-1),]
w      <- rmvnorm( 1 , mean = mu , sigma = D )
x[k,]  <- A %*% AM( x[(k-1),] ) + B %*% u.temp + AM(w[(k-1),])
}
}
compute.states()
N  = 100
x0 = c(3,2)
D  = diag(2) * c(0.5,0.2)
R  = diag(2) * c( 2, 1 )
A  = matrix( c(0,3,0,0), 2, 2)
B  = matrix( c(4,2,0,1), 2, 2)
C  = c( 5, 0 )
# Set mean for disturbance
mu      <- c( 0 , 0 )
# Initialize storage objets and set corresponding starting conditions
x       <- matrix( NA , nrow = N , ncol = 2 )
x[1,]   <- x0
Q       <- C %*% t(C)
K       <- list()
K[[N]]  <- Q
L       <- list()
# Compute K and L matrices
for (k in (N-1):1){
K[[k]] <-   t(A) %*% ( K[[(k+1)]] - K[[(k+1)]] %*% B %*% solve( t(B) %*% K[[(k+1)]] %*% B + R ) %*% t(B) %*% K[[(k+1)]] ) %*% A + Q
L[[k]] <- - solve( t(B) %*% K[[(k+1)]] %*% B + R) %*% t(B) %*% K[[(k+1)]] %*% A
}
# Solve for the states
w      <- rmvnorm( 100 , mean = mu , sigma = D )
for( k in 2:N ){
u.temp <- L[[(k - 1)]] %*% x[(k-1),]
w      <- rmvnorm( 1 , mean = mu , sigma = D )
x[k,]  <- A %*% AM( x[(k-1),] ) + B %*% u.temp + AM(w[(k-1),])
}
w
w      <- rmvnorm( 100 , mean = mu , sigma = D )
w
for( k in 2:N ){
u.temp <- L[[(k - 1)]] %*% x[(k-1),]
N  = 100
x0 = c(3,2)
D  = diag(2) * c(0.5,0.2)
R  = diag(2) * c( 2, 1 )
A  = matrix( c(0,3,0,0), 2, 2)
B  = matrix( c(4,2,0,1), 2, 2)
C  = c( 5, 0 )
# Set mean for disturbance
mu      <- c( 0 , 0 )
# Initialize storage objets and set corresponding starting conditions
x       <- matrix( NA , nrow = N , ncol = 2 )
x[1,]   <- x0
Q       <- C %*% t(C)
K       <- list()
K[[N]]  <- Q
L       <- list()
# Compute K and L matrices
for (k in (N-1):1){
K[[k]] <-   t(A) %*% ( K[[(k+1)]] - K[[(k+1)]] %*% B %*% solve( t(B) %*% K[[(k+1)]] %*% B + R ) %*% t(B) %*% K[[(k+1)]] ) %*% A + Q
L[[k]] <- - solve( t(B) %*% K[[(k+1)]] %*% B + R) %*% t(B) %*% K[[(k+1)]] %*% A
}
# Solve for the states
w      <- rmvnorm( 100 , mean = mu , sigma = D )
for( k in 2:N ){
u.temp <- L[[(k - 1)]] %*% x[(k-1),]
w      <- rmvnorm( 1 , mean = mu , sigma = D )
x[k,]  <- A %*% AM( x[(k-1),] ) + B %*% u.temp + AM(w[(k-1),])
}
}
compute.states()
N  = 100
x0 = c(3,2)
D  = diag(2) * c(0.5,0.2)
R  = diag(2) * c( 2, 1 )
A  = matrix( c(0,3,0,0), 2, 2)
B  = matrix( c(4,2,0,1), 2, 2)
C  = c( 5, 0 )
# Set mean for disturbance
mu      <- c( 0 , 0 )
# Initialize storage objets and set corresponding starting conditions
x       <- matrix( NA , nrow = N , ncol = 2 )
x[1,]   <- x0
Q       <- C %*% t(C)
K       <- list()
K[[N]]  <- Q
L       <- list()
# Compute K and L matrices
for (k in (N-1):1){
K[[k]] <-   t(A) %*% ( K[[(k+1)]] - K[[(k+1)]] %*% B %*% solve( t(B) %*% K[[(k+1)]] %*% B + R ) %*% t(B) %*% K[[(k+1)]] ) %*% A + Q
L[[k]] <- - solve( t(B) %*% K[[(k+1)]] %*% B + R) %*% t(B) %*% K[[(k+1)]] %*% A
}
# Solve for the states
w      <- rmvnorm( 100 , mean = mu , sigma = D )
for( k in 2:N ){
u.temp <- L[[(k - 1)]] %*% x[(k-1),]
x[k,]  <- A %*% AM( x[(k-1),] ) + B %*% u.temp + AM(w[(k-1),])
}
}
compute.states()
w      <- rmvnorm( 100 , mean = mu , sigma = D )
w
for( k in 2:N ){
u.temp <- L[[(k - 1)]] %*% x[(k-1),]
x[k,]  <- A %*% AM( x[(k-1),] ) + B %*% u.temp + AM(w[(k-1),])
}
x
matrix( c(0,3,0,0), 2, 2)
matrix( c(4,2,0,1), 2, 2)
install.packages("TMVA")
100000000*1000
100000000*1000
setwd('/Users/felix/Documents/GSE/Term 3/14D009 Social and Economic/Networks/Problemsets/14D009-Social-and-Economic-Networks/')
setwd('/Users/felix/Documents/GSE/Term 3/14D009 Social and Economic Networks/Problemsets/14D009-Social-and-Economic-Networks/')
setwd('/Users/felix/Documents/GSE/Term 3/14D009 Social and Economic Networks/Problemsets/14D009-Social-and-Economic-Networks/')
library(igraph)
g <- sample_bipartite(10, 5, p=.4)
H = hub_score(gp)$value
gp = bipartite.projection(g)$proj1
E = eigen_centrality(gp)$value
H ; E
H = hub_score(gp)$value
H ; E
round(H,3) == round(E,3)
round(H,3) == round(E^2,3)
probas = c(.01, .1, .25, .6)
for (p in probas) {
print(paste('For probability =', p, ':'))
n = 5
g = erdos.renyi.game(n, p)
avg = average.path.length(g)
print(paste('    - average distance =', avg))
d = get.diameter(g)
print(paste('    - diameter =', length(d) - 1))
D <- lower.tri(shortest.paths(g))
D[D==Inf] <- 0
print(paste('    - the asbolute difference between the average distance and its approximation is', abs(avg-sum(D)/(n*(n-1))),'.'))
print(paste('    - the asbolute difference between the diamter and its approximation is', abs(avg-max(D)),'.'))
d <- degree_distribution(g)
pdf(paste0('plot_p', p, '.pdf'))
plot(d, type='l')
dev.off()
cl = clusters(g)
print(paste('    - number of components =', cl$no))
print('    - we see in the following table:')
t = table(cl$csize)
print(t)
print('')
print(paste('    that the largest component captures', ifelse(max(as.numeric(rownames(as.matrix((t)))))/n > .8, 'more', 'less'), 'than 80 percent of the nodes.'))
print('')
print('')
}
probas = c(.01, .1, .25, .6)
for (p in probas) {
cat(
"----------------------------------------------------------------------------------------------","\n",
'For probability = ', p,':',sep = '',"\n",
"----------------------------------------------------------------------------------------------","\n"
)
# Construct  the graph and compute average path length
n   = 5
g   = erdos.renyi.game(n, p)
# Average path length
avg = average.path.length(g)
cat('    - average distance =', avg,"\n")
# Diameter
d = get.diameter(g)
cat('    - diameter =', length(d) - 1,"\n")
#
D <- lower.tri(shortest.paths(g))
D[D==Inf] <- 0
cat('    - the asbolute difference between the average distance and its approximation is', abs(avg-sum(D)/(n*(n-1))),"\n")
cat('    - the asbolute difference between the diamter and its approximation is', abs(avg-max(D)))
d <- degree_distribution(g)
pdf(paste0('plot_p', p, '.pdf'))
plot(d, type='l')
dev.off()
plot(d, type='l')
# Find components
cl = clusters(g)
cat('    - number of components =', cl$no,"\n")
t = table(cl$csize)
cat('    - we see in the following table:',t,"\n")
cat('    ...that the largest component captures', ifelse(max(as.numeric(rownames(as.matrix((t)))))/n > .8, 'more', 'less'), 'than 80 percent of the nodes.',"\n")
cat("\n","\n")
}
d <- read.table('/Users/felix/Documents/GSE/Term 3/14D009 Social and Economic Networks/Problemsets/14D009-Social-and-Economic-Networks/ca-AstroPh.txt', header=T)
a <- graph.adjlist(as.matrix(d))
dim(a)
length(a)
class(a)
g <- make_ring(10)
al <- as_adj_list(g)
a <- as_adj_list(g)
a1 = centr_degree
a2 = eigen_centrality
a3 = page_rank
a4 = authority.score
#... their values
a1(a)$centralization
class(a)
a <- graph.adjlist(g)
library(igraph)
get.adjacency(graph.edgelist(as.matrix(dat), directed=FALSE))
apply(m, c(1,2), function(x) sample(c(0,1),1))
n <- 10
k <- 10
dim(d)
a <- graph.adjlist(as.matrix(d)[1:100,])
a1 = centr_degree
a2 = eigen_centrality
a3 = page_rank
a4 = authority.score
#... their values
a1(a)$centralization
a2(a)$value
a3(a)$value
a4(a)$value
#... and the 10 most central instances according to each of them
c1 = a1(a)$res
c2 = a2(a)$vector
c3 = a3(a)$vector
c4 = a4(a)$vector
maxC <- function(v) {unlist(as.matrix(sort.int(v, decreasing=T, index.return=TRUE))[2])[1:10]}
print('degree centrality')
maxC(c1)
cat('Degree Centrality:',"\n",maxC(c1))
cat('Eigen Centrality: ',"\n",maxC(c2))
cat('Page Rank: ',"\n",maxC(c3))
cat('HITS: ',"\n",maxC(c4))
g <- graph_from_edgelist(as.matrix(d), directed = F)
d <- degree_distribution(g)
plot(d, type='l', ylim=range(0,.01))
plot(d, type='l', ylim=range(0,.01),xlab="Degree",ylab="Fraction of nodes")
n <- 133279
cl <- clusters(g)
print(paste('    - number of components =', cl$no))
print('    - we see in the following table:')
t = table(cl$csize)
print(t)
print('')
print(paste('    that the largest component captures', round(17903/n*100, 0), 'percent of the nodes.'))
print('But this cluster is still much bigger than the second one.')
print(paste('There are', sum(adjacent.triangles(g)), 'adjacent triangles.'))
cat'    - number of components =', cl$no)
cat('    - number of components =', cl$no)
cat('    that the largest component captures', round(17903/n*100, 0), 'percent of the nodes.')
cat('There are', sum(adjacent.triangles(g)), 'adjacent triangles.')
setwd('/Users/felix/Documents/GSE/Term 3/14D009 Social and Economic Networks/Problemsets/14D009-Social-and-Economic-Networks/')
library(igraph)
#1.2
g <- sample_bipartite(10, 5, p=.4)
gp = bipartite.projection(g)$proj1
H = hub_score(gp)$value
E = eigen_centrality(gp)$value
H ; E
round(H,3) == round(E,3)
round(H,3) == round(E^2,3)
#2.1
probas = c(.01, .1, .25, .6)
for (p in probas) {
print(paste('For probability =', p, ':'))
n = 5
g = erdos.renyi.game(n, p)
avg = average.path.length(g)
print(paste('    - average distance =', avg))
d = get.diameter(g)
print(paste('    - diameter =', length(d) - 1))
D <- lower.tri(shortest.paths(g))
D[D==Inf] <- 0
print(paste('    - the asbolute difference between the average distance and its approximation is', abs(avg-sum(D)/(n*(n-1))),'.'))
print(paste('    - the asbolute difference between the diamter and its approximation is', abs(avg-max(D)),'.'))
d <- degree_distribution(g)
pdf(paste0('plot_p', p, '.pdf'))
plot(d, type='l')
dev.off()
cl = clusters(g)
print(paste('    - number of components =', cl$no))
print('    - we see in the following table:')
t = table(cl$csize)
print(t)
print('')
print(paste('    that the largest component captures', ifelse(max(as.numeric(rownames(as.matrix((t)))))/n > .8, 'more', 'less'), 'than 80 percent of the nodes.'))
print('')
print('')
}
library(igraph)
#1.2
g <- sample_bipartite(10, 5, p=.4)
gp = bipartite.projection(g)$proj1
H = hub_score(gp)$value
E = eigen_centrality(gp)$value
H ; E
round(H,3) == round(E,3)
round(H,3) == round(E^2,3)
#2.1
probas = c(.01, .1, .25, .6)
for (p in probas) {
print(paste('For probability =', p, ':'))
n = 5
g = erdos.renyi.game(n, p)
avg = average.path.length(g)
print(paste('    - average distance =', avg))
d = get.diameter(g)
print(paste('    - diameter =', length(d) - 1))
D <- lower.tri(shortest.paths(g))
D[D==Inf] <- 0
print(paste('    - the asbolute difference between the average distance and its approximation is', abs(avg-sum(D)/(n*(n-1))),'.'))
print(paste('    - the asbolute difference between the diamter and its approximation is', abs(avg-max(D)),'.'))
d <- degree_distribution(g)
pdf(paste0('plot_p', p, '.pdf'))
plot(d, type='l')
dev.off()
plot(d, type='l')
cl = clusters(g)
print(paste('    - number of components =', cl$no))
print('    - we see in the following table:')
t = table(cl$csize)
print(t)
print('')
print(paste('    that the largest component captures', ifelse(max(as.numeric(rownames(as.matrix((t)))))/n > .8, 'more', 'less'), 'than 80 percent of the nodes.'))
print('')
print('')
}
library(igraph)
#1.2
g <- sample_bipartite(10, 5, p=.4)
gp = bipartite.projection(g)$proj1
H = hub_score(gp)$value
E = eigen_centrality(gp)$value
H ; E
round(H,3) == round(E,3)
round(H,3) == round(E^2,3)
probas = c(.01, .1, .25, .6)
for (p in probas) {
cat(
"----------------------------------------------------------------------------------------------","\n",
'For probability = ', p,':',sep = '',"\n",
"----------------------------------------------------------------------------------------------","\n"
)
# Construct  the graph and compute average path length
n   = 10
g   = erdos.renyi.game(n, p)
# Average path length
avg = average.path.length(g)
cat('    - average distance =', avg,"\n")
# Diameter
d = get.diameter(g)
cat('    - diameter =', length(d) - 1,"\n")
#
D <- lower.tri(shortest.paths(g))
D[D==Inf] <- 0
cat('    - the asbolute difference between the average distance and its approximation is', abs(avg-  log(n)/log(p*(n-1)) ),"\n")
cat('    - the asbolute difference between the diamter and its approximation is', abs(avg-max(D)))
d <- degree_distribution(g)
plot(d, type='l', xlab="Degree",ylab="Fraction of nodes")
d02 <- degree(g,mode="in")
# Find components
cl = clusters(g)
cat('    - number of components =', cl$no,"\n")
t = table(cl$csize)
cat('    - we see in the following table:',t,"\n")
cat('    ...that the largest component captures', ifelse(max(as.numeric(rownames(as.matrix((t)))))/n > .8, 'more', 'less'), 'than 80 percent of the nodes.',"\n")
cat("\n","\n")
}
d02 <- degree(g,mode="in")
d02
plot(d02)
plot(d02,type="l")
pl <- fit_power_law(d02+1)
plot(pl)
pl
fit_power_law = function(graph) {
# calculate degree
d = degree(graph, mode = "all")
dd = degree.distribution(graph, mode = "all", cumulative = FALSE)
degree = 1:max(d)
probability = dd[-1]
# delete blank values
nonzero.position = which(probability != 0)
probability = probability[nonzero.position]
degree = degree[nonzero.position]
reg = lm(log(probability) ~ log(degree))
cozf = coef(reg)
power.law.fit = function(x) exp(cozf[[1]] + cozf[[2]] * log(x))
alpha = -cozf[[2]]
R.square = summary(reg)$r.squared
print(paste("Alpha =", round(alpha, 3)))
print(paste("R square =", round(R.square, 3)))
# plot
plot(probability ~ degree, log = "xy", xlab = "Degree (log)", ylab = "Probability (log)",
col = 1, main = "Degree Distribution")
curve(power.law.fit, col = "red", add = T, n = length(d))
}
fit_power_law(g)
